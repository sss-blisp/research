# -*- coding: utf-8 -*-
"""RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QhqH3g_6r9Td1KiVhqRG6kQMifUb06oq
"""

from google.colab import drive
drive.mount('/content/drive')

"""### pip"""

pip install matplotlib seaborn torch torchvision torcheval

!pip install shap

import pandas as pd
from sklearn import preprocessing

import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from torcheval.metrics.functional.classification import multiclass_recall

"""### 関数"""

def pd_read(number):

  test = []
  train = []
  # testデータ用
  for i in range(2):

    test_df = pd.read_csv('/content/drive/My Drive/' +str(number) +'/test'+str(i)+'.csv') #クラスタリング
    test_df, df_x  = Preprocessing(test_df)
    test.append(test_df)

  test_loaders = Time_series(test)



  # #クラスタリング
  for i in range(20):

    train_df = pd.read_csv('/content/drive/My Drive/' +str(number) +'/train'+str(i)+'.csv')
    train_df, _ = Preprocessing(train_df)
    train.append(train_df)

  train_loaders = Time_series(train)


  # print(data[: , :,:, 0].shape, label.shape)
  return test_loaders, train_loaders, df_x

def Preprocessing(test_df):
  df_x = test_df[
    [

       'AU01_r','AU02_r','AU04_r','AU05_r','AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r', 'AU15_r', 'AU17_r', 'AU20_r', 'AU23_r', 'AU25_r', 'AU26_r', 'AU45_r',
        'dis_right_outer', 'dis_right_inner', 'dis_left_outer', 'dis_left_inner',
        'dis_eyelids_right', 'dis_eyelids_left',
        'pose_Rx', 'pose_Ry', 'pose_Rz'

# 'AU04_r','AU07_r','AU17_r','AU25_r', 'dis_left_outer', 'dis_eyelids_right', 'pose_Rx', 'pose_Ry', 'pose_Rz'
    ]
  ]
  # print(len(df_x))

  # 正規化
  ss = preprocessing.StandardScaler()
  df_x1 = ss.fit_transform(df_x)
  df_x = pd.DataFrame(df_x1, columns=df_x.columns)
  # print(df_x_test)


  df_y = test_df['pain'] # ラベルエンコーディング
  df_y = df_y.reset_index(drop=True)

  df = pd.concat([df_y, df_x], axis=1)

  return df,df_x

### (2)入力が7つでチャネルが2つの時(test): 下のセルと同じ

def Time_series(dataframe):

  ####
  # 初期設定
  # 時系列データの取得数　今回は5に設定
  sequence_size = 5

  n_sample = len(dataframe[0]) - sequence_size - 1

  input_data = np.zeros((n_sample, sequence_size, 7,len(dataframe))) #(44, 5, 7,2) #変更
  #input_data = np.zeros((n_sample, sequence_size, 7)) #(44,5,7)

  correct_data = np.zeros((n_sample, 1,len(dataframe)))

  for df_index, df in enumerate(dataframe):

    df_x = df[[

'AU04_r','AU06_r','AU07_r','AU09_r','AU10_r', 'dis_eyelids_right', 'dis_eyelids_left'
    ]]
    df_y = df['pain']

    for i in range(n_sample):

      #input_data[i] = df_x_test[i:i+sequence_size].values  # (5,7)
      #input_data[i] = df_x_test[i:i+sequence_size].values.reshape(5,7,1)
      input_data[i, :, :, df_index] = df_x[i:i+sequence_size].values #(5, 7) -> (5,7,2)

      # print(input_data[i, :, :, df_index])
      # print('-------------------------------')
      # print(input_data[i, :, :, df_index].shape)


      correct_data[i, : ,df_index] = df_y.iloc[i+sequence_size] # (1, 2)
  # print(input_data[: , :,:, 1])
  # print(correct_data.shape)
  # print(correct_data[:,: ,1])


  input_data = torch.tensor(input_data, dtype=torch.float32)
  correct_data = torch.tensor(correct_data, dtype=torch.float32)

  # dataset = torch.utils.data.TensorDataset(input_data, correct_data)

  # チャネルごとでデータセットを作り、リストに格納する
  loaders = []
  # バッチサイズ(1回の学習で使用するデータ数)
  batch_size = 4 #データ数44
  for i in range(len(dataframe)):
    dataset = torch.utils.data.TensorDataset(input_data[: , : , : , i], correct_data[: , : ,i])
    loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, drop_last=True)
    loaders.append(loader)

  return loaders

### (2)入力が7つでチャネルが62つの時(train)

def Time_series_train(train):

  ####
  # 初期設定
  # 時系列データの取得数　今回は5に設定
  sequence_size = 5

  n_sample = len(train[0]) - sequence_size - 1

  input_data = np.zeros((n_sample, sequence_size, 7,len(train))) #(44, 5, 7,62) #変更 62->人数*2？

  correct_data = np.zeros((n_sample, 1, len(train))) #変更 62->人数*2？

  for df_index, df in enumerate(train):

    df_x_train = df[[

'AU04_r','AU06_r','AU07_r','AU09_r','AU10_r', 'dis_eyelids_right', 'dis_eyelids_left'
 ]]
    df_y_train = df['pain']

    for i in range(n_sample):

      #input_data[i] = df_x_test[i:i+sequence_size].values  # (5,7)

      input_data[i, :, :, df_index] = df_x_train[i:i+sequence_size].values #(5, 7) -> (5,7,62)

      # print(input_data[i, :, :, df_index])
      # print('-------------------------------')
      # print(input_data[i, :, :, df_index].shape)


      correct_data[i, : ,df_index] = df_y_train.iloc[i+sequence_size] # (1, 62)

  # print(correct_data.shape)

  input_data = torch.tensor(input_data, dtype=torch.float32)
  correct_data = torch.tensor(correct_data, dtype=torch.float32)

  # チャネルごとでデータセットを作り、リストに格納する
  train_loaders = []
  # バッチサイズ(1回の学習で使用するデータ数)
  batch_size = 4 #データ数44
  for i in range(len(train)): #変更 62->人数*2？
    dataset = torch.utils.data.TensorDataset(input_data[: , : , : , i], correct_data[: , : ,i])
    train_loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=False, drop_last=True)
    train_loaders.append(train_loader)

  return train_loaders

acc = []
loss = []
recall = []

"""### 学習モデルの作成"""

import torch.nn as nn

# 今回学習させるモデル
# nn.RNNへの入力は(sequence_size,batch_size,input_size)
class Net(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        outputs, h = self.rnn(x)
        output = self.fc(h)
        output = self.sigmoid(output)


        return output

import torch.optim as optim

train_acc_list = []
train_loss_list = []

def train(train_loaders):
  for i in range(1, epochs+1):
    acc_list = []
    loss_list = []

    # 学習部
    net.train() #学習モード
    running_loss =0.0

    # train繰り返す
    for train_loader in train_loaders:
      # optimizer.zero_grad() #勾配を初期化
      for _, (x, t) in enumerate(train_loader):

        x = x.to(device) #GPUへ
        optimizer.zero_grad() #勾配を初期化

        y = net(x) #RNNで予測

        y = y.to('cpu') #予測結果をCPUに戻す
        y = y.view(4, 1)


        y_label =torch.zeros((4,1))

        for k, row in enumerate(y):

          value = row.item()
          if value <=0.5:
            y_label[k] = 0
          else:
            y_label[k] = 1


        acc = torch.sum(y_label == t) * 1.0 / len(t)

        acc_list.append(acc) # 正解率をリストに保存

        loss = loss_fnc(y, t) #平均二乗誤差

        loss_list.append(loss.item())

        loss.backward()  #逆伝番

        optimizer.step()  #勾配を更新


    train_acc_list.append(sum(acc_list) / len(acc_list))
    train_loss_list.append(sum(loss_list) / len(loss_list))

"""### calc"""

def calc_acc(data_loader):
  with torch.no_grad():
    accs = []

  for num , test_loader in enumerate(test_loaders):
    for j, (x, t) in enumerate(test_loader):
      x = x.to(device) #GPUへ
      y = net(x)
      y = y.to('cpu') #予測結果をCPUに戻す

      y = y.view(4, 1)

      y_label =torch.zeros((4,1))

      for k, row in enumerate(y):

        value = row.item()
        if value <=0.5:
          y_label[k] = 0
        else:
          y_label[k] = 1

      acc = torch.sum(y_label == t) * 1.0 / len(t)
      accs.append(acc)

  avg_acc = torch.tensor(accs).mean()

  return avg_acc

def calc_loss(test_loaders):
  with torch.no_grad():
    losses = []
    for num , test_loader in enumerate(test_loaders):
      for j, (x, t) in enumerate(test_loader):
        x = x.to(device) #GPUへ
        y = net(x)
        y = y.to('cpu') #予測結果をCPUに戻す
        loss = loss_fnc(y, t)
        losses.append(loss.item())

  avg_loss = torch.tensor(losses).mean()

  return avg_loss

# バッチごとで毎回、recallを算出
def calc_recall(data_loader):
  with torch.no_grad():
    recalls = []


    for num , test_loader in enumerate(test_loaders):
      for j, (x, t) in enumerate(test_loader):
        x = x.to(device) #GPUへ
        y = net(x)
        y = y.to('cpu') #予測結果をCPUに戻す

        y = y.view(4, 1)

        y_label =torch.zeros((4,1))

        for k, row in enumerate(y):

          value = row.item()
          if value <=0.5:
            y_label[k] = 0
          else:
            y_label[k] = 1

          recall = multiclass_recall(y_label[k], t[k])
          recalls.append(recall)

    recall_acc = torch.tensor(recalls).mean()

    return recall_acc

"""### 実行"""

inputs_list = []
shap_torch_list = []

for i in range(11):
  input_size = 7 #変更
  hidden_size = 4
  output_size = 1

  net = Net(input_size,hidden_size,output_size)

  # config
  loss_fnc = nn.MSELoss() # 平均二乗誤差
  optimizer = optim.SGD(net.parameters(), lr=0.001) #確率的勾配降下
  device = torch.device("cuda:0" if torch.cuda. is_available() else "cpu")  #(GPU or CPU)設定
  epochs = 100 #エポック数

  net.to(device) #モデルをGPU(CPU)へ

  loss_record = []

  print('-------------------------'+str(i)+ '回目' + '-----------------------------')
  test_loaders, train_loaders ,df_x= pd_read(i)

  train(train_loaders)


  test_acc = calc_acc(test_loaders)
  test_loss = calc_loss(test_loaders)
  test_recall = calc_recall(test_loaders)

  print('テストデータでの正解率:' +str(test_acc)+'\n')
  print('テストデータでのloss:' +str(test_loss)+'\n')
  print('テストデータでのrecall:' +str(test_recall)+'\n')


  acc.append(test_acc)
  loss.append(test_loss)
  recall.append(test_recall)

print(sum(acc)/len(acc))
print(sum(loss)/len(loss))
print(sum(recall)/len(recall))

"""### shap用"""

# 配列を重ね合わせて平均値を計算
average_array = np.mean(shap_torch_list, axis=0)
print("\n重ね合わせた後の平均値:")
print(type(average_array))
print(average_array.shape)
average_array = list(average_array)
print(type(average_array))
print(type(average_array[0]))

# リストのテンソルを重ね合わせる
stacked = torch.stack(inputs_list, dim=0)

# 平均値を計算
average_ = torch.mean(stacked, dim=0)

print("\n平均値:")
print(average_.shape)

shap.summary_plot(average_array,average_, feature_names=df_x.columns.values, axis_color='#000000', show=False,plot_type="bar")

a = [0.9773,0.5795]

"""### plot"""

plt.plot(train_acc_list)
plt.xlabel("epochs")
plt.ylabel("train")
plt.show()

plt.plot(train_loss_list)
plt.xlabel("epochs")
plt.ylabel("loss")
plt.show()

"""### shap"""

import shap
shap_ex = shap.DeepExplainer

def shp(test_loader , df_x):
  net.eval()
  for batch in test_loader:
      inputs, targets = batch
  # DeepExplainer for approximating the shap values
  print(net)
  explainer = shap_ex(net, inputs) # torch.nn.Module instance
                                # [torch.tensor]
  print(type(inputs))
  print(inputs.shape)
  shap_values = explainer.shap_values(inputs, check_additivity=False)
  print("aaa")
  a = np.array(shap_values)

# print(a.size) #全要素数 (2, 25, 6) (クラス、バッチ数、説明変数の数)

# shap.summary_plot(shap_values, inputs, feature_names=df_x.columns.values, axis_color='#000000', show=False,plot_type="bar")
  return inputs, a