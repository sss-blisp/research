# -*- coding: utf-8 -*-
"""NNshap説明変数_全部.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ojlQpKZj0v_w6eLnchDRGtHgOfoHooXE

NNモデルの詳細

４層の全結合型
(25,10) →.活性化関数 →(10,5) →.活性化関数→ (5,2)

ハイパーパラメータ

*   バッチサイズ : 25
*   エポック数 200 → 100
*   学習率 : 0.001

最適化関数 : Adam

活性化関数: Relu関数

損失関数: クロスエントロピー

データの詳細

１秒間に約30fps

10秒の動画なので、１本の動画から300フレームを獲得する

今回は全てのフレームを活用した

関数化にして自動にする

pip install
"""

pip install --upgrade shap

pip install japanize_matplotlib

!pip install shap

pip install matplotlib seaborn torch torchvision torcheval

from google.colab import drive
drive.mount('/content/drive')

!pip install shap
import shap


import pandas as pd
from sklearn import preprocessing

import torch
import torch.nn as nn
import torch.nn.functional as F

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from torch.utils.data import DataLoader

from sklearn import preprocessing

import torch
from torcheval.metrics.functional.classification import binary_recall
from torcheval.metrics.functional.classification import multiclass_recall

from sklearn.metrics import confusion_matrix

import matplotlib.pyplot as plt

import japanize_matplotlib

import shap

# 最適化関数間でのグラフを表示する用
acc_SGD = []
recall_SGD = []
loss_SGD = []

acc_Adam = []
recall_Adam = []
loss_loss = []

# データセット間でのグラフを表示する用
all_acc = []
all_recall = []
all_loss = []


# データ読み込み
def pd_read(number):
  test_df = pd.read_csv('/content/drive/My Drive/test_'+str(number)+'.csv')
  train_df = pd.read_csv('/content/drive/My Drive/train_'+str(number)+'.csv')

  return test_df, train_df

# 前処理（学習データ）
def Preprocessing_train(train_df):
  # 説明変数
  df_x = train_df[
      [
      'AU12_r', 'AU17_r', 'AU10_r', 'AU25_r', 'dis_eyelids_right', 'dis_eyelids_left'
      ]
  ]
  # 目的変数
  df_y = train_df['pain'] 
  df = pd.concat([df_y, df_x], axis=1)

  # 正規化
  ss = preprocessing.StandardScaler()

  df_x1 = ss.fit_transform(df_x)
  df_x = pd.DataFrame(df_x1, columns=df_x.columns)

  df_y = df['pain']
  df = pd.concat([df_y, df_x], axis=1)

  x = torch.tensor(df_x.values, dtype=torch.float32)
  t = torch.tensor(df_y.values, dtype=torch.int64)

  dataset = torch.utils.data.TensorDataset(x, t)

  # trainデータセット、valデータセット、testデータセットに格納するデータの数を計算する
  n_train = int(len(dataset) * 0.8)
  n_val = int(len(dataset) - n_train)

  # 乱数を固定する
  # colabが時間切れになり再実行しても同じ結果を得られるようにする
  torch.manual_seed(0)

  # 元のデータセットからそれぞれに分割する
  train, val = torch.utils.data.random_split(dataset, [n_train, n_val])

  batch_size = 25 

  train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True, drop_last=True)
  val_loader = torch.utils.data.DataLoader(val, batch_size)

  return train_loader, val_loader

# 前処理（テストデータ）
def Preprocessing_test(test_df):
  df_x = test_df[
    [
    'AU12_r', 'AU17_r', 'AU10_r', 'AU25_r', 'dis_eyelids_right', 'dis_eyelids_left'
    ]
  ]
  df_y = test_df['pain'] 
  df_test = pd.concat([df_y, df_x], axis=1)

  ss = preprocessing.StandardScaler()
  df_x1 = ss.fit_transform(df_x)
  df_x_test = pd.DataFrame(df_x1, columns=df_x.columns)

  df_y_test = df_test['pain'] 
  df_y_test = df_y.reset_index(drop=True)

  df_test = pd.concat([df_y_test, df_x_test], axis=1)

  x = torch.tensor(df_x.values, dtype=torch.float32)
  t = torch.tensor(df_y.values, dtype=torch.int64)

  dataset_test = torch.utils.data.TensorDataset(x, t)

  batch_size = 25 
  test_loader = torch.utils.data.DataLoader(dataset_test, batch_size, drop_last=True)
  return test_loader, df_x

# バッチごとで正解率を算出する
def calc_acc(data_loader):
  with torch.no_grad():
    accs = []

    for batch in data_loader:
      x, t = batch # data_loader内の一個のデータが入る
      y = net(x) # ニューラルネットワーククラスでの計算結果を格納する
      y_label = torch.argmax(y, dim=1) #dim=1で行ごとの最大値に対する要素番号を取得
      acc = torch.sum(y_label == t) * 1.0 / len(t)
      accs.append(acc)
    avg_acc = torch.tensor(accs).mean()

    return avg_acc

# バッチごとで再現率を算出する
def calc_recall(data_loader):
  with torch.no_grad():
    recalls = []
    y_for_recall =[]
    t_for_recall = []

    for batch in data_loader:
      x, t = batch # data_loader内の一個のデータが入る
      y = net(x) # ニューラルネットワーククラスでの計算結果を格納する

      y_label = torch.argmax(y, dim=1) #dim=1で行ごとの最大値に対する要素番号を取得

      recall = multiclass_recall(y_label, t)

      recalls.append(recall)
    avg_recall = torch.tensor(recalls).mean()

    return avg_recall

def calc_loss(data_loader):
  with torch.no_grad():
    losses = []
    for batch in data_loader:
      x, t = batch
      y = net(x)
      loss = criterion(y, t)
      losses.append(loss.item())

    avg_loss = torch.tensor(losses).mean()

    return avg_loss

class NeuralNetwork(nn.Module): #nn.Moduleパッケージを継承した独自の派生クラス、torch.nn.Moduleクラスのサブクラス化
                                 #nn.Moduleを継承
  def __init__(self):
    super(NeuralNetwork, self).__init__()
    self.fc1 = nn.Linear(6, 4)
    self.fc2 = nn.Linear(4, 2)
    self.fc3 = nn.Linear(4, 2)

  def forward(self, x):
    x = self.fc1(x)
    x = F.relu(x)
    x = self.fc2(x)
    return x

criterion = F.cross_entropy # 正解との誤差を計算する誤差関数

def train(train_loader, val_loader, test_loader):

  max_epoch = 100

  train_loss_list = []
  train_acc_list = []
  traiin_recall_list = []

  val_acc_list = []
  val_loss_list = []
  val_recall_list = []

  test_acc_list = []
  test_loss_list = []
  test_recall_list = []

  net.train() 
  for epoch in range(max_epoch):
    acc_list = []
    loss_list = []
    recall_list = []

    # 学習用のデータセットのループ
    for batch in train_loader:
      x, t = batch
      optimizer.zero_grad() #勾配を初期化
      y = net(x) #__call__メソッド

      y_label = torch.argmax(y, dim=1)
      acc = torch.sum(y_label == t) * 1.0 / len(t)
      acc_list.append(acc) # 正解率をリストに保存

      # -------
      recall = multiclass_recall(y_label, t)
      recall_list.append(recall)
      # -------
      loss = criterion(y, t) #関数呼び出し
      #print(type(loss))
      loss_list.append(loss.item()) # 誤差をリストに保存

      loss.backward() # 誤差から勾配を計算
      optimizer.step() # 勾配から重みの更新

    train_acc_list.append(sum(acc_list) / len(acc_list))
    train_loss_list.append(sum(loss_list) / len(loss_list))
    traiin_recall_list.append(sum(recall_list) / len(recall_list))

    val_acc = calc_acc(val_loader)
    val_acc_list.append(val_acc)
    val_loss = calc_loss(val_loader)
    val_loss_list.append(val_loss)

    val_recall = calc_recall(val_loader)
    val_recall_list.append(val_recall)

    test_acc = calc_acc(test_loader)
    test_acc_list.append(test_acc)

    test_loss = calc_loss(test_loader)
    test_loss_list.append(test_loss)

    test_recall = calc_recall(test_loader)
    test_recall_list.append(test_recall)


  all_acc.append(test_acc_list)
  all_recall.append(test_recall_list)
  all_loss.append(test_loss_list)

  return   train_loss_list, train_acc_list , traiin_recall_list , val_acc_list, val_loss_list , val_recall_list , test_acc_list , test_loss_list , test_recall_list



# train,val,testのrecallのグラフを出力する
def recall_graph(train_recall_list, val_recall_list, test_recall_list):

  plt.plot(train_recall_list, label='train')
  plt.plot(val_recall_list, label='valid')
  plt.plot(test_recall_list, label='test')

  plt.legend()

  # x軸にラベルを追加
  plt.xlabel('Epoch')

  #y軸
  plt.ylabel('Recall')

  # グラフを表示
  plt.show()



# train,val,testのaccのグラフを出力する
def acc_graph(train_recall_list, val_recall_list, test_recall_list):
  plt.plot(train_acc_list, label='train')
  plt.plot(val_acc_list, label='valid')
  plt.plot(test_acc_list, label='test')

  plt.legend()

  # x軸にラベルを追加
  plt.xlabel('Epoch')

  #y軸
  plt.ylabel('Accuracy')

  # グラフを表示
  plt.show()

# train,val,testのlossのグラフを出力する
def loss_graph(train_recall_list, val_recall_list, test_recall_list):
  plt.plot(train_loss_list, label='train')
  plt.plot(val_loss_list, label='valid')
  plt.plot(test_loss_list, label='test')

  plt.legend()

  # x軸にラベルを追加
  plt.xlabel('Epoch')

  #y軸
  plt.ylabel('Loss')

  # グラフを表示
  plt.show()

inputs_list = []
shap_torch_list = []

# 被験者32人のため
for i in range(32):
  net = NeuralNetwork()
  optimizer = torch.optim.Adam(net.parameters(), lr=0.001) # 最適化関数(確率的勾配降下法)
  print('-------------------------'+str(i)+ '回目' + '-----------------------------')
  test_df, train_df = pd_read(i)
  train_loader, val_loader = Preprocessing_train(train_df)
  test_loader, df_x = Preprocessing_test(test_df)


my_list = [2, 25, 6]
print(np.array(my_list).shape)

def acc_list(all_acc):
  for i, acc in enumerate(all_acc):
    print('-------------'+str(i)+'------------------')
    print(acc)
acc_list(all_acc)



acc_mean = []
for a in all_acc:

  acc_mean.append(a[-1])

mean = np.mean(acc_mean)
print(mean)

print('----------')
recall_mean = []
for a in all_recall:

  recall_mean.append(a[-1])

mean = np.mean(recall_mean)
print(mean)

print('----------')
loss_mean = []
for a in all_loss:

  loss_mean.append(a[-1])

mean = np.mean(loss_mean)
print(mean)
